{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "nmt_torch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "assured-roulette"
      },
      "source": [
        "import torchtext"
      ],
      "id": "assured-roulette",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stylish-valentine"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random"
      ],
      "id": "stylish-valentine",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "variable-clarity"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "variable-clarity",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comfortable-lesbian"
      },
      "source": [
        "###### load tokenizers for english and german"
      ],
      "id": "comfortable-lesbian"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brief-driving"
      },
      "source": [
        "%%capture\n",
        "# !python -m spacy download de_core_news_sm\n",
        "# !python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de"
      ],
      "id": "brief-driving",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "genuine-phase"
      },
      "source": [
        "import en_core_web_sm\n",
        "import de_core_news_sm\n",
        "\n",
        "spacy_ger = de_core_news_sm.load()\n",
        "spacy_eng = en_core_web_sm.load()"
      ],
      "id": "genuine-phase",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "interpreted-causing"
      },
      "source": [
        "def tokenizer_ger(german_text):\n",
        "    return [tok.text for tok in spacy_ger.tokenizer(german_text)]\n",
        "\n",
        "def tokenizer_eng(english_text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(english_text)]"
      ],
      "id": "interpreted-causing",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laden-camera"
      },
      "source": [
        "# German and English Preprocessing\n",
        "german = Field(tokenize=tokenizer_ger, lower=True,\n",
        "              init_token='<START>', eos_token='<END>')\n",
        "\n",
        "english = Field(tokenize=tokenizer_eng, lower=True,\n",
        "              init_token='<START>', eos_token='<END>')"
      ],
      "id": "laden-camera",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "municipal-edinburgh"
      },
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
        "                                                        fields=(german, english))"
      ],
      "id": "municipal-edinburgh",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufficient-investing"
      },
      "source": [
        "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "id": "sufficient-investing",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parallel-resort"
      },
      "source": [
        "# LSTM\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_num):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(drop_num)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_num)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        \n",
        "        return hidden, cell"
      ],
      "id": "parallel-resort",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confidential-gospel"
      },
      "source": [
        "# LSTM\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, drop_num):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(drop_num)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_num)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(0) # predict one word at a time\n",
        "        \n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        \n",
        "        predictions = self.fc(outputs)\n",
        "        \n",
        "        predictions = predictions.squeeze(0)\n",
        "        \n",
        "        return predictions, hidden, cell\n",
        "        "
      ],
      "id": "confidential-gospel",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infectious-running"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(english.vocab)\n",
        "        \n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        \n",
        "        hidden, cell = self.encoder(source)\n",
        "        \n",
        "        # take the start token\n",
        "        x = target[0]\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            \n",
        "            outputs[t] = output\n",
        "            \n",
        "            best_guess = output.argmax(1)\n",
        "            \n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "            \n",
        "        return outputs"
      ],
      "id": "infectious-running",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "administrative-matthew"
      },
      "source": [
        "batch_size = 64\n",
        "lr = 0.001"
      ],
      "id": "administrative-matthew",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharing-pipeline"
      },
      "source": [
        "input_size_encoder = len(german.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "num_layers=2\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "\n",
        "hidden_size = 1024\n",
        "\n",
        "drop_rate = 0.5"
      ],
      "id": "sharing-pipeline",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spread-italian"
      },
      "source": [
        "train_iter, valid_iter, test_iter = BucketIterator.splits((train_data, validation_data, test_data), \n",
        "                                                           batch_size = batch_size,\n",
        "                                                           sort_within_batch = True,\n",
        "                                                           sort_key = lambda x : len(x.src),\n",
        "                                                           device=device)"
      ],
      "id": "spread-italian",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blank-enzyme"
      },
      "source": [
        "encoder_model = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, drop_rate).to(device)"
      ],
      "id": "blank-enzyme",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "according-captain"
      },
      "source": [
        "decoder_model = Decoder(input_size_decoder, decoder_embedding_size, \n",
        "                        hidden_size, output_size, num_layers, drop_rate).to(device)"
      ],
      "id": "according-captain",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "considerable-bunch"
      },
      "source": [
        "model = Seq2Seq(encoder_model, decoder_model).to(device)"
      ],
      "id": "considerable-bunch",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focused-secretariat"
      },
      "source": [
        "pad_idx = english.vocab.stoi['<PAD>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # did not know this was allowed"
      ],
      "id": "focused-secretariat",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "horizontal-century"
      },
      "source": [
        "### Training Phase"
      ],
      "id": "horizontal-century"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eight-finger"
      },
      "source": [
        "def train(model, num_epochs, optimizer):\n",
        "    lowest = 0\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        cumulative_loss = 0\n",
        "        for batch_idx, batch in enumerate(train_iter):\n",
        "            input_data = batch.src.to(device)\n",
        "            target = batch.trg.to(device)\n",
        "            \n",
        "            output = model(input_data, target)\n",
        "            \n",
        "            output = output[1:].reshape(-1, output.shape[2]) # do not consider start token\n",
        "            target = target[1:].reshape(-1)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "            \n",
        "            cumulative_loss += loss.item()\n",
        "        # print(cumulative_loss)\n",
        "        if(epoch == 0):\n",
        "          lowest = cumulative_loss\n",
        "        elif (cumulative_loss < lowest):\n",
        "          lowest = cumulative_loss\n",
        "          torch.save(model.state_dict(), 'seqtoseq.pt')\n",
        "            \n",
        "            "
      ],
      "id": "eight-finger",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bright-edward"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train(model, 20, optimizer)"
      ],
      "id": "bright-edward",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infinite-disney"
      },
      "source": [
        "def translate(model, sentence, german, english, device, max_length=50):\n",
        "  spacy_ger = de_core_news_sm.load()\n",
        "  spacy_eng = en_core_web_sm.load()\n",
        "\n",
        "  if type(sentence) == str:\n",
        "    tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "  else:\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "  tokens.insert(0, german.init_token)\n",
        "  tokens.append(german.eos_token)\n",
        "\n",
        "  text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "  sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "  outputs = [english.vocab['<sos>']]\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "      best_guess = output.argmax(1).item()\n",
        "\n",
        "    outputs.append(best_guess)\n",
        "\n",
        "    if output.argmax(1).item() == english.vocab.stoi['<eos>']:\n",
        "      break\n",
        "\n",
        "\n",
        "  translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "  return translated_sentence[1:]"
      ],
      "id": "infinite-disney",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S53aAJlPWc3D",
        "outputId": "7c872974-4314-448c-b825-d453790c4e55"
      },
      "source": [
        "model.load_state_dict(torch.load('seqtoseq.pt'))"
      ],
      "id": "S53aAJlPWc3D",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(7855, 300)\n",
              "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(5893, 300)\n",
              "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=5893, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QvLACGeUVhZ",
        "outputId": "c81ed999-bac6-4579-e046-1d1edd7b165d"
      },
      "source": [
        "model.eval()\n",
        "sentence = 'ein Boot mit anderen MÃ¤nnern.'\n",
        "print(translate(model, sentence, german, english, device, max_length=50))"
      ],
      "id": "4QvLACGeUVhZ",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['men', 'with', 'two', 'men', 'men', '.', '<END>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnG4HaRHjswm"
      },
      "source": [
        ""
      ],
      "id": "jnG4HaRHjswm",
      "execution_count": null,
      "outputs": []
    }
  ]
}