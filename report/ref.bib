@article{scalingNMT,
  author    = {Myle Ott and
               Sergey Edunov and
               David Grangier and
               Michael Auli},
  title     = {Scaling Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1806.00187},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00187},
  archivePrefix = {arXiv},
  eprint    = {1806.00187},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00187.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{surveylowresourcenmt,
  author={D. {Liu} and N. {Ma} and F. {Yang} and X. {Yang}},
  booktitle={2019 4th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)}, 
  title={A Survey of Low Resource Neural Machine Translation}, 
  year={2019},
  volume={},
  number={},
  pages={39-393},
  doi={10.1109/ICMCCE48743.2019.00017}
  }
@article{lowresourcenmtasian,
author = {Rubino, Raphael and Marie, Benjamin and Dabre, Raj and Fujita, Atsushi and Utiyama, Masao and Sumita, Eiichiro},
year = {2020},
month = {12},
pages = {},
title = {Extremely low-resource neural machine translation for Asian languages},
volume = {34},
journal = {Machine Translation},
doi = {10.1007/s10590-020-09258-6}
}
@misc{alignandtranslate,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{seqtoseq,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{nmtrevandsurvey,
      title={Neural Machine Translation: A Review and Survey}, 
      author={Felix Stahlberg},
      year={2020},
      eprint={1912.02047},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{seq2seqtorch, 
    title={NLP From Scratch: Translation with a Sequence to Sequence Network and Attention¶}, url={https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\#the-seq2seq-model}, journal={NLP From Scratch: Translation with a Sequence to Sequence Network and Attention - PyTorch Tutorials 1.8.1+cu102 documentation}, author={Robertson, Sean}
} 
@inproceedings{bleu,
  title={Bleu: a Method for Automatic Evaluation of Machine Translation},
  author={Kishore Papineni and S. Roukos and T. Ward and Wei-Jing Zhu},
  booktitle={ACL},
  year={2002}
}

@article{gru,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{googlenmt,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  archivePrefix = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{pruningandquant,
      title={Pruning and Quantization for Deep Neural Network Acceleration: A Survey}, 
      author={Tailin Liang and John Glossner and Lei Wang and Shaobo Shi},
      year={2021},
      eprint={2101.09671},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{stochasticparrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ��},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
@article{energypolicy,
  author    = {Emma Strubell and
               Ananya Ganesh and
               Andrew McCallum},
  title     = {Energy and Policy Considerations for Deep Learning in {NLP}},
  journal   = {CoRR},
  volume    = {abs/1906.02243},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.02243},
  archivePrefix = {arXiv},
  eprint    = {1906.02243},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-02243.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{systematic,
      title={Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning}, 
      author={Peter Henderson and Jieru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
      year={2020},
      eprint={2002.05651},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

 @misc{googlebleu,
 title={Evaluating models &nbsp;|&nbsp; AutoML Translation Documentation &nbsp;|&nbsp; Google Cloud}, url={https://cloud.google.com/translate/automl/docs/evaluate}, 
 journal={Google}, 
 publisher={Google}
 } 
 
@misc{compilingonnx,
      title={Compiling ONNX Neural Network Models Using MLIR}, 
      author={Tian Jin and Gheorghe-Teodor Bercea and Tung D. Le and Tong Chen and Gong Su and Haruki Imai and Yasushi Negishi and Anh Leu and Kevin O'Brien and Kiyokuni Kawachiya and Alexandre E. Eichenberger},
      year={2020},
      eprint={2008.08272},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}
@misc{switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2021},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{sgd,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@INPROCEEDINGS{cpssurvey,
  author={Rawung, Ricky Henry and Putrada, Aji Gautama},
  booktitle={2014 International Conference on ICT For Smart Society (ICISS)}, 
  title={Cyber physical system: Paper survey}, 
  year={2014},
  volume={},
  number={},
  pages={273-278},
  doi={10.1109/ICTSS.2014.7013187}
}
@inproceedings{cityresolver,
author = {Ma, Meiyi and Stankovic, John A. and Feng, Lu},
title = {Cityresolver: A Decision Support System for Conflict Resolution in Smart Cities},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCPS.2018.00014},
doi = {10.1109/ICCPS.2018.00014},
abstract = {Resolution of conflicts across services in smart cities is an important yet challenging problem. We present CityResolver - a decision support system for conflict resolution in smart cities. CityResolver uses an Integer Linear Programming based method to generate a small set of resolution options, and a Signal Temporal Logic based verification approach to compute these resolution options' impact on city performance. The trade-offs between resolution options are shown in a dashboard to support decision makers in selecting the best resolution. We demonstrate the effectiveness of CityResolver by comparing the performance with two baselines: a smart city without conflict resolution, and CityGuard which uses a priority rule-based conflict resolution. Experimental results show that CityResolver can reduce the number of requirement violations and improve the city performance significantly.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {55–64},
numpages = {10},
keywords = {signal temporal logic, conflict resolution, smart city, safety and performance requirements, smart services},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@INPROCEEDINGS{cpsbots,  
    author={Baby, Cyril Joe and Khan, Faizan Ayyub and Swathi, J. N.}, 
    booktitle={2017 Innovations in Power and Advanced Computing Technologies (i-PACT)},   
    title={Home automation using IoT and a chatbot using natural language processing},  
    year={2017}, 
    volume={},  
    number={},  
    pages={1-6},  
    doi={10.1109/IPACT.2017.8245185}
}

@Article{edgeml,
AUTHOR = {Merenda, Massimo and Porcaro, Carlo and Iero, Demetrio},
TITLE = {Edge Machine Learning for AI-Enabled IoT Devices: A Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {2533},
URL = {https://www.mdpi.com/1424-8220/20/9/2533},
ISSN = {1424-8220},
ABSTRACT = {In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors’ data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning “Hello World”.},
DOI = {10.3390/s20092533}
}
@article{nnembedded,
  author    = {Shuochao Yao and
               Yiran Zhao and
               Aston Zhang and
               Lu Su and
               Tarek F. Abdelzaher},
  title     = {Compressing Deep Neural Network Structures for Sensing Systems with
               a Compressor-Critic Framework},
  journal   = {CoRR},
  volume    = {abs/1706.01215},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.01215},
  archivePrefix = {arXiv},
  eprint    = {1706.01215},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/YaoZZSA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{federated,
  author    = {H. Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Blaise Ag{\"{u}}era y Arcas},
  title     = {Federated Learning of Deep Networks using Model Averaging},
  journal   = {CoRR},
  volume    = {abs/1602.05629},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05629},
  archivePrefix = {arXiv},
  eprint    = {1602.05629},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/McMahanMRA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}